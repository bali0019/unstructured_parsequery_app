# Databricks Asset Bundle configuration for Document Intelligence: Unstructured ParseQuery
# This file is used when deploying the app via Databricks Asset Bundles (DABs)
# Requires Databricks CLI v0.278.0 or later

bundle:
  name: unstructured-parsequery-app

# Sync configuration - exclude local dev files from upload
sync:
  exclude:
    - .venv/
    - venv/
    - env/
    - __pycache__/
    - "*.pyc"
    - "*.pyo"
    - .idea/
    - .vscode/
    - .databricks/
    - mlruns/
    - .DS_Store
    - "*.log"
    - .git/
    - .gitignore
    - pdfs/
    - CLAUDE.md
    - claude.md
    - IMPLEMENTATION_NOTES.md
    - deploy.sh
    - "*.egg-info/"
    - generate_test_pdfs.py

# Variables for customization
variables:
  # Unity Catalog location
  catalog:
    description: "Unity Catalog catalog name"
    default: users
  schema:
    description: "Unity Catalog schema name"
    default: my_schema

  # Existing infrastructure (typically shared/pre-provisioned)
  sql_warehouse_id:
    description: "SQL Warehouse ID for ai_parse_document"
    default: "your-warehouse-id"
  serving_endpoint_name:
    description: "Model serving endpoint for ai_query"
    default: "databricks-gpt-5-1"

  # Lakebase configuration
  database_instance_name:
    description: "Lakebase database instance name"
    default: "your-lakebase-instance"
  database_name:
    description: "Lakebase database name"
    default: "my_database"

# Define resources to be created/managed by DAB
resources:
  # Unity Catalog volumes (created in existing schema)
  volumes:
    # Volume for document uploads and processing
    source_volume:
      name: unstructured_parsequery_source
      catalog_name: ${var.catalog}
      schema_name: ${var.schema}
      volume_type: MANAGED
      comment: "Source documents for Unstructured ParseQuery pipeline"

    # Volume for pipeline execution logs
    logs_volume:
      name: unstructured_parsequery_logs
      catalog_name: ${var.catalog}
      schema_name: ${var.schema}
      volume_type: MANAGED
      comment: "Pipeline execution logs for Unstructured ParseQuery"

  # App definition
  apps:
    unstructured_parsequery_app:
      name: unstructured-parsequery-app
      description: "Document Intelligence: Unstructured ParseQuery - Document processing with Databricks AI Functions and MLflow Tracing"
      source_code_path: .

      # App resources - grant permissions and inject environment variables
      resources:
        # Reference the DAB-created source volume
        - name: source-volume
          description: "UC volume for document storage and processing"
          uc_securable:
            securable_full_name: "${var.catalog}.${var.schema}.${resources.volumes.source_volume.name}"
            securable_type: "VOLUME"
            permission: "WRITE_VOLUME"

        # Reference existing SQL Warehouse
        - name: parse-warehouse
          description: "SQL Warehouse for ai_parse_document"
          sql_warehouse:
            id: ${var.sql_warehouse_id}
            permission: "CAN_USE"

        # Reference existing Model Serving Endpoint
        - name: ai-query-endpoint
          description: "Model serving endpoint for AI query stages"
          serving_endpoint:
            name: ${var.serving_endpoint_name}
            permission: "CAN_QUERY"

        # Reference the DAB-created logs volume
        - name: logs-volume
          description: "UC volume for pipeline logs"
          uc_securable:
            securable_full_name: "${var.catalog}.${var.schema}.${resources.volumes.logs_volume.name}"
            securable_type: "VOLUME"
            permission: "WRITE_VOLUME"

        # Reference existing Lakebase database
        - name: database
          description: "Lakebase PostgreSQL database"
          database:
            instance_name: ${var.database_instance_name}
            database_name: ${var.database_name}
            permission: "CAN_CONNECT_AND_CREATE"

# Deployment targets
targets:
  dev:
    default: true
    variables:
      catalog: my_catalog
      schema: my_schema

  # Example production target (customize as needed)
  # prod:
  #   workspace:
  #     host: https://your-prod-workspace.cloud.databricks.com
  #   variables:
  #     catalog: main
  #     schema: production
  #     sql_warehouse_id: "your-prod-warehouse-id"
  #     database_instance_name: "prod-lakebase-instance"
  #     database_name: "prod_database"
